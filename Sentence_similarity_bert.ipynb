{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence_similarity_bert.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryanshu/NLP/blob/master/Sentence_similarity_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUsV_OFtUnL4",
        "colab_type": "code",
        "outputId": "0fb632bc-b3e4-48c0-c306-88f2e60dcf21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggXUKzt9bDqa",
        "colab_type": "code",
        "outputId": "484c30df-27bf-4b51-c432-e5a2457a394c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd \"/gdrive/My Drive/Colab Notebooks\""
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6OhBye9dhCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9b011cf8-ea88-49c1-f9b8-9a146e7ab43c"
      },
      "source": [
        "!unzip question-pairs-dataset.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  question-pairs-dataset.zip\n",
            "  inflating: questions.csv           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5kw1fj4bbiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2t8dHSXawKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('questions.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRDnd8HobWut",
        "colab_type": "code",
        "outputId": "c21cb012-3d8b-475e-cd50-18144803826c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  ...                                          question2 is_duplicate\n",
              "0   0     1  ...  What is the step by step guide to invest in sh...            0\n",
              "1   1     3  ...  What would happen if the Indian government sto...            0\n",
              "2   2     5  ...  How can Internet speed be increased by hacking...            0\n",
              "3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n",
              "4   4     9  ...            Which fish would survive in salt water?            0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiM81NQ4SOHP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "240579d2-b443-4ee4-e36f-678e4b37fe4f"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 404351 entries, 0 to 404350\n",
            "Data columns (total 6 columns):\n",
            "id              404351 non-null int64\n",
            "qid1            404351 non-null int64\n",
            "qid2            404351 non-null int64\n",
            "question1       404350 non-null object\n",
            "question2       404349 non-null object\n",
            "is_duplicate    404351 non-null int64\n",
            "dtypes: int64(4), object(2)\n",
            "memory usage: 18.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BVvoDK17AK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbxI8ECwSURx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "032a17f2-499d-4c5f-d7f6-cb178a8a3515"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 404348 entries, 0 to 404350\n",
            "Data columns (total 6 columns):\n",
            "id              404348 non-null int64\n",
            "qid1            404348 non-null int64\n",
            "qid2            404348 non-null int64\n",
            "question1       404348 non-null object\n",
            "question2       404348 non-null object\n",
            "is_duplicate    404348 non-null int64\n",
            "dtypes: int64(4), object(2)\n",
            "memory usage: 21.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHHlMYdJbhWx",
        "colab_type": "code",
        "outputId": "d5ae4eb9-96e8-4a97-ef25-7352a8bfd1fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():   \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcRJ5tdcbvGE",
        "colab_type": "code",
        "outputId": "1362ef1a-ae06-4bd4-cb27-8cbc8d485bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqxTY8e-b1eE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FASanpWvd-u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTf4Rl8K8DDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_n=df[100000:110000]\n",
        "df=df[:100000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCE19TS48bNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent1=df.question1.values\n",
        "sent2=df.question2.values\n",
        "labels=df.is_duplicate.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfsPjB_jDrs4",
        "colab_type": "code",
        "outputId": "789551a2-68ec-47fb-cab1-f82d5a1a6d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "input_ids = []\n",
        "tokenize_text=[]\n",
        "for i in range(len(sent1)):\n",
        "    \n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent1[i],\n",
        "                        sent2[i],                     \n",
        "                        add_special_tokens = True, \n",
        "                        max_length=64\n",
        "            \n",
        "                   )\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "print('Original: ', sent1[0],sent2[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "      \n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  What is the step by step guide to invest in share market in india? What is the step by step guide to invest in share market?\n",
            "Token IDs: [101, 2054, 2003, 1996, 3357, 2011, 3357, 5009, 2000, 15697, 1999, 3745, 3006, 1999, 2634, 1029, 102, 2054, 2003, 1996, 3357, 2011, 3357, 5009, 2000, 15697, 1999, 3745, 3006, 1029, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbEZxvn-AOkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LEN = 64\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1plL6m3CH2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "attention_masks = []\n",
        "\n",
        "\n",
        "for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq6d76PKJXpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E2jbcdBJabP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqD_eaOtJc-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5W-m2TcJgc-",
        "colab_type": "code",
        "outputId": "31bd2468-c5b4-4eeb-b55a-afc1e09366fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1FqIYMRLcij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut5ggqh5JnV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 3\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfChofZ0JqSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzRnTJwEJtMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    \n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbUo-b3fJv-9",
        "colab_type": "code",
        "outputId": "230b2850-061f-48f4-9874-8dd11dc0aaee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "   \n",
        "        model.train()\n",
        " \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "   \n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "        logits = outputs[0]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    torch.save(model.state_dict(), \"models/sent_bert1.pth\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  2,813.    Elapsed: 0:00:09.\n",
            "  Batch    80  of  2,813.    Elapsed: 0:00:19.\n",
            "  Batch   120  of  2,813.    Elapsed: 0:00:28.\n",
            "  Batch   160  of  2,813.    Elapsed: 0:00:38.\n",
            "  Batch   200  of  2,813.    Elapsed: 0:00:47.\n",
            "  Batch   240  of  2,813.    Elapsed: 0:00:57.\n",
            "  Batch   280  of  2,813.    Elapsed: 0:01:06.\n",
            "  Batch   320  of  2,813.    Elapsed: 0:01:15.\n",
            "  Batch   360  of  2,813.    Elapsed: 0:01:25.\n",
            "  Batch   400  of  2,813.    Elapsed: 0:01:34.\n",
            "  Batch   440  of  2,813.    Elapsed: 0:01:43.\n",
            "  Batch   480  of  2,813.    Elapsed: 0:01:53.\n",
            "  Batch   520  of  2,813.    Elapsed: 0:02:02.\n",
            "  Batch   560  of  2,813.    Elapsed: 0:02:12.\n",
            "  Batch   600  of  2,813.    Elapsed: 0:02:21.\n",
            "  Batch   640  of  2,813.    Elapsed: 0:02:30.\n",
            "  Batch   680  of  2,813.    Elapsed: 0:02:40.\n",
            "  Batch   720  of  2,813.    Elapsed: 0:02:49.\n",
            "  Batch   760  of  2,813.    Elapsed: 0:02:59.\n",
            "  Batch   800  of  2,813.    Elapsed: 0:03:08.\n",
            "  Batch   840  of  2,813.    Elapsed: 0:03:17.\n",
            "  Batch   880  of  2,813.    Elapsed: 0:03:27.\n",
            "  Batch   920  of  2,813.    Elapsed: 0:03:36.\n",
            "  Batch   960  of  2,813.    Elapsed: 0:03:45.\n",
            "  Batch 1,000  of  2,813.    Elapsed: 0:03:55.\n",
            "  Batch 1,040  of  2,813.    Elapsed: 0:04:04.\n",
            "  Batch 1,080  of  2,813.    Elapsed: 0:04:13.\n",
            "  Batch 1,120  of  2,813.    Elapsed: 0:04:23.\n",
            "  Batch 1,160  of  2,813.    Elapsed: 0:04:32.\n",
            "  Batch 1,200  of  2,813.    Elapsed: 0:04:42.\n",
            "  Batch 1,240  of  2,813.    Elapsed: 0:04:51.\n",
            "  Batch 1,280  of  2,813.    Elapsed: 0:05:00.\n",
            "  Batch 1,320  of  2,813.    Elapsed: 0:05:10.\n",
            "  Batch 1,360  of  2,813.    Elapsed: 0:05:19.\n",
            "  Batch 1,400  of  2,813.    Elapsed: 0:05:28.\n",
            "  Batch 1,440  of  2,813.    Elapsed: 0:05:38.\n",
            "  Batch 1,480  of  2,813.    Elapsed: 0:05:47.\n",
            "  Batch 1,520  of  2,813.    Elapsed: 0:05:56.\n",
            "  Batch 1,560  of  2,813.    Elapsed: 0:06:06.\n",
            "  Batch 1,600  of  2,813.    Elapsed: 0:06:15.\n",
            "  Batch 1,640  of  2,813.    Elapsed: 0:06:24.\n",
            "  Batch 1,680  of  2,813.    Elapsed: 0:06:33.\n",
            "  Batch 1,720  of  2,813.    Elapsed: 0:06:43.\n",
            "  Batch 1,760  of  2,813.    Elapsed: 0:06:52.\n",
            "  Batch 1,800  of  2,813.    Elapsed: 0:07:01.\n",
            "  Batch 1,840  of  2,813.    Elapsed: 0:07:11.\n",
            "  Batch 1,880  of  2,813.    Elapsed: 0:07:20.\n",
            "  Batch 1,920  of  2,813.    Elapsed: 0:07:29.\n",
            "  Batch 1,960  of  2,813.    Elapsed: 0:07:39.\n",
            "  Batch 2,000  of  2,813.    Elapsed: 0:07:48.\n",
            "  Batch 2,040  of  2,813.    Elapsed: 0:07:57.\n",
            "  Batch 2,080  of  2,813.    Elapsed: 0:08:07.\n",
            "  Batch 2,120  of  2,813.    Elapsed: 0:08:16.\n",
            "  Batch 2,160  of  2,813.    Elapsed: 0:08:25.\n",
            "  Batch 2,200  of  2,813.    Elapsed: 0:08:34.\n",
            "  Batch 2,240  of  2,813.    Elapsed: 0:08:44.\n",
            "  Batch 2,280  of  2,813.    Elapsed: 0:08:53.\n",
            "  Batch 2,320  of  2,813.    Elapsed: 0:09:02.\n",
            "  Batch 2,360  of  2,813.    Elapsed: 0:09:12.\n",
            "  Batch 2,400  of  2,813.    Elapsed: 0:09:21.\n",
            "  Batch 2,440  of  2,813.    Elapsed: 0:09:30.\n",
            "  Batch 2,480  of  2,813.    Elapsed: 0:09:40.\n",
            "  Batch 2,520  of  2,813.    Elapsed: 0:09:49.\n",
            "  Batch 2,560  of  2,813.    Elapsed: 0:09:58.\n",
            "  Batch 2,600  of  2,813.    Elapsed: 0:10:08.\n",
            "  Batch 2,640  of  2,813.    Elapsed: 0:10:17.\n",
            "  Batch 2,680  of  2,813.    Elapsed: 0:10:26.\n",
            "  Batch 2,720  of  2,813.    Elapsed: 0:10:36.\n",
            "  Batch 2,760  of  2,813.    Elapsed: 0:10:45.\n",
            "  Batch 2,800  of  2,813.    Elapsed: 0:10:54.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:10:57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:21\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  2,813.    Elapsed: 0:00:09.\n",
            "  Batch    80  of  2,813.    Elapsed: 0:00:19.\n",
            "  Batch   120  of  2,813.    Elapsed: 0:00:28.\n",
            "  Batch   160  of  2,813.    Elapsed: 0:00:37.\n",
            "  Batch   200  of  2,813.    Elapsed: 0:00:47.\n",
            "  Batch   240  of  2,813.    Elapsed: 0:00:56.\n",
            "  Batch   280  of  2,813.    Elapsed: 0:01:05.\n",
            "  Batch   320  of  2,813.    Elapsed: 0:01:15.\n",
            "  Batch   360  of  2,813.    Elapsed: 0:01:24.\n",
            "  Batch   400  of  2,813.    Elapsed: 0:01:33.\n",
            "  Batch   440  of  2,813.    Elapsed: 0:01:43.\n",
            "  Batch   480  of  2,813.    Elapsed: 0:01:52.\n",
            "  Batch   520  of  2,813.    Elapsed: 0:02:01.\n",
            "  Batch   560  of  2,813.    Elapsed: 0:02:11.\n",
            "  Batch   600  of  2,813.    Elapsed: 0:02:20.\n",
            "  Batch   640  of  2,813.    Elapsed: 0:02:29.\n",
            "  Batch   680  of  2,813.    Elapsed: 0:02:39.\n",
            "  Batch   720  of  2,813.    Elapsed: 0:02:48.\n",
            "  Batch   760  of  2,813.    Elapsed: 0:02:57.\n",
            "  Batch   800  of  2,813.    Elapsed: 0:03:07.\n",
            "  Batch   840  of  2,813.    Elapsed: 0:03:16.\n",
            "  Batch   880  of  2,813.    Elapsed: 0:03:25.\n",
            "  Batch   920  of  2,813.    Elapsed: 0:03:35.\n",
            "  Batch   960  of  2,813.    Elapsed: 0:03:44.\n",
            "  Batch 1,000  of  2,813.    Elapsed: 0:03:53.\n",
            "  Batch 1,040  of  2,813.    Elapsed: 0:04:03.\n",
            "  Batch 1,080  of  2,813.    Elapsed: 0:04:12.\n",
            "  Batch 1,120  of  2,813.    Elapsed: 0:04:22.\n",
            "  Batch 1,160  of  2,813.    Elapsed: 0:04:31.\n",
            "  Batch 1,200  of  2,813.    Elapsed: 0:04:40.\n",
            "  Batch 1,240  of  2,813.    Elapsed: 0:04:50.\n",
            "  Batch 1,280  of  2,813.    Elapsed: 0:04:59.\n",
            "  Batch 1,320  of  2,813.    Elapsed: 0:05:08.\n",
            "  Batch 1,360  of  2,813.    Elapsed: 0:05:18.\n",
            "  Batch 1,400  of  2,813.    Elapsed: 0:05:27.\n",
            "  Batch 1,440  of  2,813.    Elapsed: 0:05:37.\n",
            "  Batch 1,480  of  2,813.    Elapsed: 0:05:46.\n",
            "  Batch 1,520  of  2,813.    Elapsed: 0:05:55.\n",
            "  Batch 1,560  of  2,813.    Elapsed: 0:06:05.\n",
            "  Batch 1,600  of  2,813.    Elapsed: 0:06:14.\n",
            "  Batch 1,640  of  2,813.    Elapsed: 0:06:23.\n",
            "  Batch 1,680  of  2,813.    Elapsed: 0:06:33.\n",
            "  Batch 1,720  of  2,813.    Elapsed: 0:06:42.\n",
            "  Batch 1,760  of  2,813.    Elapsed: 0:06:51.\n",
            "  Batch 1,800  of  2,813.    Elapsed: 0:07:01.\n",
            "  Batch 1,840  of  2,813.    Elapsed: 0:07:10.\n",
            "  Batch 1,880  of  2,813.    Elapsed: 0:07:19.\n",
            "  Batch 1,920  of  2,813.    Elapsed: 0:07:29.\n",
            "  Batch 1,960  of  2,813.    Elapsed: 0:07:38.\n",
            "  Batch 2,000  of  2,813.    Elapsed: 0:07:48.\n",
            "  Batch 2,040  of  2,813.    Elapsed: 0:07:57.\n",
            "  Batch 2,080  of  2,813.    Elapsed: 0:08:06.\n",
            "  Batch 2,120  of  2,813.    Elapsed: 0:08:16.\n",
            "  Batch 2,160  of  2,813.    Elapsed: 0:08:25.\n",
            "  Batch 2,200  of  2,813.    Elapsed: 0:08:34.\n",
            "  Batch 2,240  of  2,813.    Elapsed: 0:08:44.\n",
            "  Batch 2,280  of  2,813.    Elapsed: 0:08:53.\n",
            "  Batch 2,320  of  2,813.    Elapsed: 0:09:02.\n",
            "  Batch 2,360  of  2,813.    Elapsed: 0:09:12.\n",
            "  Batch 2,400  of  2,813.    Elapsed: 0:09:21.\n",
            "  Batch 2,440  of  2,813.    Elapsed: 0:09:31.\n",
            "  Batch 2,480  of  2,813.    Elapsed: 0:09:40.\n",
            "  Batch 2,520  of  2,813.    Elapsed: 0:09:49.\n",
            "  Batch 2,560  of  2,813.    Elapsed: 0:09:59.\n",
            "  Batch 2,600  of  2,813.    Elapsed: 0:10:08.\n",
            "  Batch 2,640  of  2,813.    Elapsed: 0:10:17.\n",
            "  Batch 2,680  of  2,813.    Elapsed: 0:10:27.\n",
            "  Batch 2,720  of  2,813.    Elapsed: 0:10:36.\n",
            "  Batch 2,760  of  2,813.    Elapsed: 0:10:45.\n",
            "  Batch 2,800  of  2,813.    Elapsed: 0:10:55.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:10:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:21\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  2,813.    Elapsed: 0:00:09.\n",
            "  Batch    80  of  2,813.    Elapsed: 0:00:19.\n",
            "  Batch   120  of  2,813.    Elapsed: 0:00:28.\n",
            "  Batch   160  of  2,813.    Elapsed: 0:00:37.\n",
            "  Batch   200  of  2,813.    Elapsed: 0:00:47.\n",
            "  Batch   240  of  2,813.    Elapsed: 0:00:56.\n",
            "  Batch   280  of  2,813.    Elapsed: 0:01:05.\n",
            "  Batch   320  of  2,813.    Elapsed: 0:01:15.\n",
            "  Batch   360  of  2,813.    Elapsed: 0:01:24.\n",
            "  Batch   400  of  2,813.    Elapsed: 0:01:34.\n",
            "  Batch   440  of  2,813.    Elapsed: 0:01:43.\n",
            "  Batch   480  of  2,813.    Elapsed: 0:01:52.\n",
            "  Batch   520  of  2,813.    Elapsed: 0:02:01.\n",
            "  Batch   560  of  2,813.    Elapsed: 0:02:11.\n",
            "  Batch   600  of  2,813.    Elapsed: 0:02:20.\n",
            "  Batch   640  of  2,813.    Elapsed: 0:02:29.\n",
            "  Batch   680  of  2,813.    Elapsed: 0:02:39.\n",
            "  Batch   720  of  2,813.    Elapsed: 0:02:48.\n",
            "  Batch   760  of  2,813.    Elapsed: 0:02:57.\n",
            "  Batch   800  of  2,813.    Elapsed: 0:03:07.\n",
            "  Batch   840  of  2,813.    Elapsed: 0:03:16.\n",
            "  Batch   880  of  2,813.    Elapsed: 0:03:25.\n",
            "  Batch   920  of  2,813.    Elapsed: 0:03:35.\n",
            "  Batch   960  of  2,813.    Elapsed: 0:03:44.\n",
            "  Batch 1,000  of  2,813.    Elapsed: 0:03:53.\n",
            "  Batch 1,040  of  2,813.    Elapsed: 0:04:03.\n",
            "  Batch 1,080  of  2,813.    Elapsed: 0:04:12.\n",
            "  Batch 1,120  of  2,813.    Elapsed: 0:04:21.\n",
            "  Batch 1,160  of  2,813.    Elapsed: 0:04:31.\n",
            "  Batch 1,200  of  2,813.    Elapsed: 0:04:40.\n",
            "  Batch 1,240  of  2,813.    Elapsed: 0:04:49.\n",
            "  Batch 1,280  of  2,813.    Elapsed: 0:04:59.\n",
            "  Batch 1,320  of  2,813.    Elapsed: 0:05:08.\n",
            "  Batch 1,360  of  2,813.    Elapsed: 0:05:17.\n",
            "  Batch 1,400  of  2,813.    Elapsed: 0:05:26.\n",
            "  Batch 1,440  of  2,813.    Elapsed: 0:05:36.\n",
            "  Batch 1,480  of  2,813.    Elapsed: 0:05:45.\n",
            "  Batch 1,520  of  2,813.    Elapsed: 0:05:54.\n",
            "  Batch 1,560  of  2,813.    Elapsed: 0:06:04.\n",
            "  Batch 1,600  of  2,813.    Elapsed: 0:06:13.\n",
            "  Batch 1,640  of  2,813.    Elapsed: 0:06:22.\n",
            "  Batch 1,680  of  2,813.    Elapsed: 0:06:32.\n",
            "  Batch 1,720  of  2,813.    Elapsed: 0:06:41.\n",
            "  Batch 1,760  of  2,813.    Elapsed: 0:06:50.\n",
            "  Batch 1,800  of  2,813.    Elapsed: 0:06:59.\n",
            "  Batch 1,840  of  2,813.    Elapsed: 0:07:09.\n",
            "  Batch 1,880  of  2,813.    Elapsed: 0:07:18.\n",
            "  Batch 1,920  of  2,813.    Elapsed: 0:07:27.\n",
            "  Batch 1,960  of  2,813.    Elapsed: 0:07:37.\n",
            "  Batch 2,000  of  2,813.    Elapsed: 0:07:46.\n",
            "  Batch 2,040  of  2,813.    Elapsed: 0:07:55.\n",
            "  Batch 2,080  of  2,813.    Elapsed: 0:08:04.\n",
            "  Batch 2,120  of  2,813.    Elapsed: 0:08:14.\n",
            "  Batch 2,160  of  2,813.    Elapsed: 0:08:23.\n",
            "  Batch 2,200  of  2,813.    Elapsed: 0:08:32.\n",
            "  Batch 2,240  of  2,813.    Elapsed: 0:08:42.\n",
            "  Batch 2,280  of  2,813.    Elapsed: 0:08:51.\n",
            "  Batch 2,320  of  2,813.    Elapsed: 0:09:00.\n",
            "  Batch 2,360  of  2,813.    Elapsed: 0:09:09.\n",
            "  Batch 2,400  of  2,813.    Elapsed: 0:09:19.\n",
            "  Batch 2,440  of  2,813.    Elapsed: 0:09:28.\n",
            "  Batch 2,480  of  2,813.    Elapsed: 0:09:37.\n",
            "  Batch 2,520  of  2,813.    Elapsed: 0:09:47.\n",
            "  Batch 2,560  of  2,813.    Elapsed: 0:09:56.\n",
            "  Batch 2,600  of  2,813.    Elapsed: 0:10:05.\n",
            "  Batch 2,640  of  2,813.    Elapsed: 0:10:15.\n",
            "  Batch 2,680  of  2,813.    Elapsed: 0:10:24.\n",
            "  Batch 2,720  of  2,813.    Elapsed: 0:10:33.\n",
            "  Batch 2,760  of  2,813.    Elapsed: 0:10:42.\n",
            "  Batch 2,800  of  2,813.    Elapsed: 0:10:52.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:10:55\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:21\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQgDGVEsuO8e",
        "colab_type": "text"
      },
      "source": [
        "##continuing training from cheakpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nw7BA52ud0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 1\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiHx_PhZrkIi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "93890add-15d4-4627-be05-e33fb7d52e28"
      },
      "source": [
        "model2=torch.load('models/sent_bert1.pth')\n",
        "model.load_state_dict(model2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeVG2W9hrVzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62ed1c72-92ac-4d86-d02f-efb857e66c77"
      },
      "source": [
        "import random\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "\n",
        "for epoch_i in range(0, 1):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "   \n",
        "        model.train()\n",
        " \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "   \n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "        logits = outputs[0]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    torch.save(model.state_dict(), \"models/sent_bert1.pth\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    40  of  2,194.    Elapsed: 0:00:10.\n",
            "  Batch    80  of  2,194.    Elapsed: 0:00:19.\n",
            "  Batch   120  of  2,194.    Elapsed: 0:00:29.\n",
            "  Batch   160  of  2,194.    Elapsed: 0:00:38.\n",
            "  Batch   200  of  2,194.    Elapsed: 0:00:48.\n",
            "  Batch   240  of  2,194.    Elapsed: 0:00:58.\n",
            "  Batch   280  of  2,194.    Elapsed: 0:01:07.\n",
            "  Batch   320  of  2,194.    Elapsed: 0:01:17.\n",
            "  Batch   360  of  2,194.    Elapsed: 0:01:26.\n",
            "  Batch   400  of  2,194.    Elapsed: 0:01:36.\n",
            "  Batch   440  of  2,194.    Elapsed: 0:01:45.\n",
            "  Batch   480  of  2,194.    Elapsed: 0:01:55.\n",
            "  Batch   520  of  2,194.    Elapsed: 0:02:04.\n",
            "  Batch   560  of  2,194.    Elapsed: 0:02:14.\n",
            "  Batch   600  of  2,194.    Elapsed: 0:02:24.\n",
            "  Batch   640  of  2,194.    Elapsed: 0:02:33.\n",
            "  Batch   680  of  2,194.    Elapsed: 0:02:43.\n",
            "  Batch   720  of  2,194.    Elapsed: 0:02:52.\n",
            "  Batch   760  of  2,194.    Elapsed: 0:03:02.\n",
            "  Batch   800  of  2,194.    Elapsed: 0:03:11.\n",
            "  Batch   840  of  2,194.    Elapsed: 0:03:21.\n",
            "  Batch   880  of  2,194.    Elapsed: 0:03:30.\n",
            "  Batch   920  of  2,194.    Elapsed: 0:03:40.\n",
            "  Batch   960  of  2,194.    Elapsed: 0:03:49.\n",
            "  Batch 1,000  of  2,194.    Elapsed: 0:03:59.\n",
            "  Batch 1,040  of  2,194.    Elapsed: 0:04:09.\n",
            "  Batch 1,080  of  2,194.    Elapsed: 0:04:18.\n",
            "  Batch 1,120  of  2,194.    Elapsed: 0:04:28.\n",
            "  Batch 1,160  of  2,194.    Elapsed: 0:04:37.\n",
            "  Batch 1,200  of  2,194.    Elapsed: 0:04:47.\n",
            "  Batch 1,240  of  2,194.    Elapsed: 0:04:56.\n",
            "  Batch 1,280  of  2,194.    Elapsed: 0:05:06.\n",
            "  Batch 1,320  of  2,194.    Elapsed: 0:05:15.\n",
            "  Batch 1,360  of  2,194.    Elapsed: 0:05:25.\n",
            "  Batch 1,400  of  2,194.    Elapsed: 0:05:34.\n",
            "  Batch 1,440  of  2,194.    Elapsed: 0:05:44.\n",
            "  Batch 1,480  of  2,194.    Elapsed: 0:05:54.\n",
            "  Batch 1,520  of  2,194.    Elapsed: 0:06:03.\n",
            "  Batch 1,560  of  2,194.    Elapsed: 0:06:13.\n",
            "  Batch 1,600  of  2,194.    Elapsed: 0:06:22.\n",
            "  Batch 1,640  of  2,194.    Elapsed: 0:06:32.\n",
            "  Batch 1,680  of  2,194.    Elapsed: 0:06:41.\n",
            "  Batch 1,720  of  2,194.    Elapsed: 0:06:51.\n",
            "  Batch 1,760  of  2,194.    Elapsed: 0:07:00.\n",
            "  Batch 1,800  of  2,194.    Elapsed: 0:07:10.\n",
            "  Batch 1,840  of  2,194.    Elapsed: 0:07:19.\n",
            "  Batch 1,880  of  2,194.    Elapsed: 0:07:29.\n",
            "  Batch 1,920  of  2,194.    Elapsed: 0:07:38.\n",
            "  Batch 1,960  of  2,194.    Elapsed: 0:07:48.\n",
            "  Batch 2,000  of  2,194.    Elapsed: 0:07:57.\n",
            "  Batch 2,040  of  2,194.    Elapsed: 0:08:07.\n",
            "  Batch 2,080  of  2,194.    Elapsed: 0:08:16.\n",
            "  Batch 2,120  of  2,194.    Elapsed: 0:08:26.\n",
            "  Batch 2,160  of  2,194.    Elapsed: 0:08:35.\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:08:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:16\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpZhnU0wJ9D1",
        "colab_type": "code",
        "outputId": "1f67f9fe-21e3-4408-fbc4-6b69fa6837aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC4IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcBqAmMW88WO",
        "colab_type": "text"
      },
      "source": [
        "## Ploting loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sBd8-5WV5cc",
        "colab_type": "text"
      },
      "source": [
        "##EVALUATING ON TEST SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVr9cZI8rwjv",
        "colab_type": "code",
        "outputId": "20b92b5d-d49d-4de3-e437-8732ab1de729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_t = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model_t.cuda()\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ipX_wpeOv9H",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgXs6bZEOgSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading saved model\n",
        "\n",
        "import torch\n",
        "model2=torch.load(\"models/sent_bert1.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqCQSjpUQk6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_n.drop(['Unnamed: 0','index'],axis=1,inplace=True)\n",
        "sent1=df_n.question1.values\n",
        "sent2=df_n.question2.values\n",
        "labels=df_n.is_duplicate.values\n",
        "\n",
        "input_ids = []\n",
        "\n",
        "tokenize_text=[]\n",
        "for i in range(len(sent1)):\n",
        "\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent1[i],\n",
        "                        sent2[i],                     \n",
        "                        add_special_tokens = True, \n",
        "                        max_length=128\n",
        "            \n",
        "                   )\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LEN = 256\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "\n",
        "attention_masks = []\n",
        "for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT_liVptPoCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "test_inputs = torch.tensor(input_ids)\n",
        "test_labels = torch.tensor(labels)\n",
        "test_masks = torch.tensor(attention_masks)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSSXfnpSrjLq",
        "colab_type": "code",
        "outputId": "e7f8f157-7fe2-42ed-befc-e3e8cccfe2a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model_t.load_state_dict(model2)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIh_SqEAqTVi",
        "colab_type": "code",
        "outputId": "e253dd16-7c95-4bcc-ad3a-e2509ff96b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model_t.eval()\n",
        " \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2egYjgUosTkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred1=predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygsTJaJtGcUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred2=[]\n",
        "for i in range(len(true_labels)):\n",
        "  pred= np.argmax(pred1[i], axis=1).flatten()\n",
        "  pred2=pred2+pred.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQccX8ZSstDF",
        "colab_type": "code",
        "outputId": "4397f99e-e3d3-4d2e-94fe-1b2e306d020a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred2=np.asarray(pred2)\n",
        "true_labels[1]"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnmT2J_Ss0fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t3=[]\n",
        "for i in range(len(true_labels)):\n",
        "  t2= true_labels[i]\n",
        "  t3=t3+t2.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3mqe_SOs3gN",
        "colab_type": "code",
        "outputId": "8957f635-faca-46be-ea29-1cdd43d953be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score \n",
        "acc=accuracy_score(pred2,t3)\n",
        "f1=f1_score(pred2,t3)\n",
        "precision=precision_score(pred2,t3)\n",
        "recall=recall_score(pred2,t3)\n",
        "print(\" accuracy={} \\n f1={} \\n precision={} \\n recall={}\".format(acc,f1,precision,recall))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " accuracy=0.8763 \n",
            " f1=0.8364405659129974 \n",
            " precision=0.8623227917121047 \n",
            " recall=0.8120667522464698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nACpRf65tAI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}