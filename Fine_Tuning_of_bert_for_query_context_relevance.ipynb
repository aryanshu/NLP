{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tuning of bert for query_context relevance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryanshu/NLP/blob/master/Fine_Tuning_of_bert_for_query_context_relevance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUsV_OFtUnL4",
        "colab_type": "code",
        "outputId": "e8391200-c33c-49cc-eefc-27a45b6f243c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggXUKzt9bDqa",
        "colab_type": "code",
        "outputId": "e735c94a-9b78-46a7-c6f0-e8bbe5d309a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd \"/gdrive/My Drive/Colab Notebooks\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5kw1fj4bbiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QupCUxIubTOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('f_train2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRDnd8HobWut",
        "colab_type": "code",
        "outputId": "1c23a835-9713-4f47-f628-6a317028aa2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>index</th>\n",
              "      <th>context</th>\n",
              "      <th>label</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>1</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>1</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>1</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>1</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>1</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  index  ... label                                           question\n",
              "0           0      0  ...     1  To whom did the Virgin Mary allegedly appear i...\n",
              "1           1      1  ...     1  What is in front of the Notre Dame Main Building?\n",
              "2           2      2  ...     1  The Basilica of the Sacred heart at Notre Dame...\n",
              "3           3      3  ...     1                  What is the Grotto at Notre Dame?\n",
              "4           4      4  ...     1  What sits on top of the Main Building at Notre...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiM81NQ4SOHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df[:60000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMkFfGNRSRiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df= df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbxI8ECwSURx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_n=df[55000:60000]\n",
        "df=df[:55000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6GydqzpSZBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df[:10000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHHlMYdJbhWx",
        "colab_type": "code",
        "outputId": "bb2052b1-efd0-41f7-b60b-e4ab1d86294e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():   \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcRJ5tdcbvGE",
        "colab_type": "code",
        "outputId": "75defa19-cbc2-4a35-ace6-96fd9e715eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 11.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=4ed0c86fef1c455a6ed3dcf9ee409dd09ef0a65dc31e09758f6b68ef988719fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqxTY8e-b1eE",
        "colab_type": "code",
        "outputId": "3540de5e-be22-4f06-bef0-ec297e9c2b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "from transformers import BertTokenizer"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FASanpWvd-u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCE19TS48bNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent1=df.context.values\n",
        "sent2=df.question.values\n",
        "labels=df.label.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tcbCu_HkKtf",
        "colab_type": "code",
        "outputId": "a85b260d-4c2e-4a4f-a1c6-2fbb0ea52181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "df.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
        "df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>context</th>\n",
              "      <th>label</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7462</td>\n",
              "      <td>The top 10 contestants started with five males...</td>\n",
              "      <td>1</td>\n",
              "      <td>How many contestants did this season have?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>120</td>\n",
              "      <td>On 15 October 1969, while paying a visit to th...</td>\n",
              "      <td>0</td>\n",
              "      <td>What was the size of the Notre Dame endowment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15325</td>\n",
              "      <td>The last use of the firing squad between 1608 ...</td>\n",
              "      <td>0</td>\n",
              "      <td>What does management of canon law aim to regul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37573</td>\n",
              "      <td>At birth, Victoria was fifth in the line of su...</td>\n",
              "      <td>1</td>\n",
              "      <td>What was Victorias place in line of succession...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11036</td>\n",
              "      <td>The 2013 Human Development Report by the Unite...</td>\n",
              "      <td>1</td>\n",
              "      <td>Which year was used for estimates in the 2013 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...                                           question\n",
              "0   7462  ...         How many contestants did this season have?\n",
              "1    120  ...  What was the size of the Notre Dame endowment ...\n",
              "2  15325  ...  What does management of canon law aim to regul...\n",
              "3  37573  ...  What was Victorias place in line of succession...\n",
              "4  11036  ...  Which year was used for estimates in the 2013 ...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH_Pfcj6tCsd",
        "colab_type": "code",
        "outputId": "70968cfc-2526-4fe8-9f18-7fd788d10aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "df.drop(['index'],axis=1,inplace=True)\n",
        "df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>label</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The top 10 contestants started with five males...</td>\n",
              "      <td>1</td>\n",
              "      <td>How many contestants did this season have?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>On 15 October 1969, while paying a visit to th...</td>\n",
              "      <td>0</td>\n",
              "      <td>What was the size of the Notre Dame endowment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The last use of the firing squad between 1608 ...</td>\n",
              "      <td>0</td>\n",
              "      <td>What does management of canon law aim to regul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>At birth, Victoria was fifth in the line of su...</td>\n",
              "      <td>1</td>\n",
              "      <td>What was Victorias place in line of succession...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The 2013 Human Development Report by the Unite...</td>\n",
              "      <td>1</td>\n",
              "      <td>Which year was used for estimates in the 2013 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  ...                                           question\n",
              "0  The top 10 contestants started with five males...  ...         How many contestants did this season have?\n",
              "1  On 15 October 1969, while paying a visit to th...  ...  What was the size of the Notre Dame endowment ...\n",
              "2  The last use of the firing squad between 1608 ...  ...  What does management of canon law aim to regul...\n",
              "3  At birth, Victoria was fifth in the line of su...  ...  What was Victorias place in line of succession...\n",
              "4  The 2013 Human Development Report by the Unite...  ...  Which year was used for estimates in the 2013 ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfsPjB_jDrs4",
        "colab_type": "code",
        "outputId": "c9716a49-e461-49d7-b51b-53b0f088a4df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "input_ids = []\n",
        "tokenize_text=[]\n",
        "for i in range(len(sent1)):\n",
        "\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent1[i],\n",
        "                        sent2[i],                     \n",
        "                        add_special_tokens = True, \n",
        "                        max_length=256\n",
        "            \n",
        "                   )\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "print('Original: ', sent1[0],sent2[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "      \n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  The top 10 contestants started with five males and five females, however, the males were eliminated consecutively in the first five weeks, with Lazaro Arbos the last male to be eliminated. For the first time in the show's history, the top 5 contestants were all female. It was also the first time that the judges' \"save\" was not used, the top four contestants were therefore given an extra week to perform again with their votes carried over with no elimination in the first week. How many contestants did this season have?\n",
            "Token IDs: [101, 1996, 2327, 2184, 10584, 2318, 2007, 2274, 3767, 1998, 2274, 3801, 1010, 2174, 1010, 1996, 3767, 2020, 5892, 5486, 2135, 1999, 1996, 2034, 2274, 3134, 1010, 2007, 2474, 9057, 2080, 12098, 15853, 1996, 2197, 3287, 2000, 2022, 5892, 1012, 2005, 1996, 2034, 2051, 1999, 1996, 2265, 1005, 1055, 2381, 1010, 1996, 2327, 1019, 10584, 2020, 2035, 2931, 1012, 2009, 2001, 2036, 1996, 2034, 2051, 2008, 1996, 6794, 1005, 1000, 3828, 1000, 2001, 2025, 2109, 1010, 1996, 2327, 2176, 10584, 2020, 3568, 2445, 2019, 4469, 2733, 2000, 4685, 2153, 2007, 2037, 4494, 3344, 2058, 2007, 2053, 9614, 1999, 1996, 2034, 2733, 1012, 102, 2129, 2116, 10584, 2106, 2023, 2161, 2031, 1029, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbEZxvn-AOkA",
        "colab_type": "code",
        "outputId": "91f9740e-b5bb-4375-891f-56c4d299a9e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LEN = 256\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1plL6m3CH2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "attention_masks = []\n",
        "\n",
        "\n",
        "for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq6d76PKJXpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E2jbcdBJabP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqD_eaOtJc-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5W-m2TcJgc-",
        "colab_type": "code",
        "outputId": "6d818e42-2f97-4f26-f658-0aa4129bb82a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1FqIYMRLcij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut5ggqh5JnV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 3\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfChofZ0JqSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzRnTJwEJtMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    \n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbUo-b3fJv-9",
        "colab_type": "code",
        "outputId": "18d172ca-54be-4d67-a447-810fd9998760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "   \n",
        "        model.train()\n",
        " \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "   \n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "        logits = outputs[0]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    torch.save(model.state_dict(), \"models/sent_bert2.pth\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    563.    Elapsed: 0:00:18.\n",
            "  Batch    80  of    563.    Elapsed: 0:00:36.\n",
            "  Batch   120  of    563.    Elapsed: 0:00:54.\n",
            "  Batch   160  of    563.    Elapsed: 0:01:12.\n",
            "  Batch   200  of    563.    Elapsed: 0:01:30.\n",
            "  Batch   240  of    563.    Elapsed: 0:01:47.\n",
            "  Batch   280  of    563.    Elapsed: 0:02:05.\n",
            "  Batch   320  of    563.    Elapsed: 0:02:23.\n",
            "  Batch   360  of    563.    Elapsed: 0:02:41.\n",
            "  Batch   400  of    563.    Elapsed: 0:02:59.\n",
            "  Batch   440  of    563.    Elapsed: 0:03:17.\n",
            "  Batch   480  of    563.    Elapsed: 0:03:34.\n",
            "  Batch   520  of    563.    Elapsed: 0:03:52.\n",
            "  Batch   560  of    563.    Elapsed: 0:04:10.\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:04:11\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    563.    Elapsed: 0:00:18.\n",
            "  Batch    80  of    563.    Elapsed: 0:00:36.\n",
            "  Batch   120  of    563.    Elapsed: 0:00:54.\n",
            "  Batch   160  of    563.    Elapsed: 0:01:12.\n",
            "  Batch   200  of    563.    Elapsed: 0:01:29.\n",
            "  Batch   240  of    563.    Elapsed: 0:01:47.\n",
            "  Batch   280  of    563.    Elapsed: 0:02:05.\n",
            "  Batch   320  of    563.    Elapsed: 0:02:23.\n",
            "  Batch   360  of    563.    Elapsed: 0:02:41.\n",
            "  Batch   400  of    563.    Elapsed: 0:02:59.\n",
            "  Batch   440  of    563.    Elapsed: 0:03:16.\n",
            "  Batch   480  of    563.    Elapsed: 0:03:34.\n",
            "  Batch   520  of    563.    Elapsed: 0:03:52.\n",
            "  Batch   560  of    563.    Elapsed: 0:04:10.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:04:11\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    563.    Elapsed: 0:00:18.\n",
            "  Batch    80  of    563.    Elapsed: 0:00:36.\n",
            "  Batch   120  of    563.    Elapsed: 0:00:54.\n",
            "  Batch   160  of    563.    Elapsed: 0:01:11.\n",
            "  Batch   200  of    563.    Elapsed: 0:01:29.\n",
            "  Batch   240  of    563.    Elapsed: 0:01:47.\n",
            "  Batch   280  of    563.    Elapsed: 0:02:05.\n",
            "  Batch   320  of    563.    Elapsed: 0:02:23.\n",
            "  Batch   360  of    563.    Elapsed: 0:02:41.\n",
            "  Batch   400  of    563.    Elapsed: 0:02:59.\n",
            "  Batch   440  of    563.    Elapsed: 0:03:16.\n",
            "  Batch   480  of    563.    Elapsed: 0:03:34.\n",
            "  Batch   520  of    563.    Elapsed: 0:03:52.\n",
            "  Batch   560  of    563.    Elapsed: 0:04:10.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:04:11\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FwsGyoTOEAB",
        "colab_type": "code",
        "outputId": "888498e3-b8a4-474b-e6d5-a119b4e57d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpZhnU0wJ9D1",
        "colab_type": "code",
        "outputId": "e16d3c01-fb1b-4cb9-b358-21b1d5c6b3a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJyIiIiJyIf8PjMsPTMA5cQoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sBd8-5WV5cc",
        "colab_type": "text"
      },
      "source": [
        "##EVALUATING ON TEST SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVr9cZI8rwjv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f75a7e60-e402-465c-e4a0-5f9201067220"
      },
      "source": [
        "model_t = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model_t.cuda()\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ipX_wpeOv9H",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgXs6bZEOgSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading saved model\n",
        "\n",
        "import torch\n",
        "model2=torch.load(\"models/sent_bert2.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqCQSjpUQk6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_n.drop(['Unnamed: 0','index'],axis=1,inplace=True)\n",
        "sent1=df_n.context.values\n",
        "sent2=df_n.question.values\n",
        "labels=df_n.label.values\n",
        "\n",
        "input_ids = []\n",
        "\n",
        "tokenize_text=[]\n",
        "for i in range(len(sent1)):\n",
        "\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent1[i],\n",
        "                        sent2[i],                     \n",
        "                        add_special_tokens = True, \n",
        "                        max_length=256\n",
        "            \n",
        "                   )\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LEN = 256\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "\n",
        "attention_masks = []\n",
        "for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT_liVptPoCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "test_inputs = torch.tensor(input_ids)\n",
        "test_labels = torch.tensor(labels)\n",
        "test_masks = torch.tensor(attention_masks)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSSXfnpSrjLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c12f364-4c82-471c-901c-7c1ab401a0e1"
      },
      "source": [
        "model_t.load_state_dict(model2)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FABg9LlJn9L7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca6c3601-38a7-4bb8-9319-4a022b34cfce"
      },
      "source": [
        "model_t.eval()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIh_SqEAqTVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a3cbf45f-5666-4003-e378-8daeda0e8015"
      },
      "source": [
        "model_t.eval()\n",
        " \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2egYjgUosTkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred1=predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhuX2sAwsqDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9498e12a-f2c5-4d22-d01c-16500b5ded7e"
      },
      "source": [
        "pred2=[]\n",
        "for i in range(len(true_labels)):\n",
        "  pred= np.argmax(pred1[i], axis=1).flatten()\n",
        "  pred2=pred2+pred.tolist()\n",
        "\n",
        "pred2"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQccX8ZSstDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f0a29b83-fec7-4fa1-c140-b61d19621e45"
      },
      "source": [
        "pred2=np.asarray(pred2)\n",
        "true_labels[1]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnmT2J_Ss0fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t3=[]\n",
        "for i in range(len(true_labels)):\n",
        "  t2= true_labels[i]\n",
        "  t3=t3+t2.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3mqe_SOs3gN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "f58f3ff9-e600-4bcf-95d3-e9e2f8c6f7e1"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score \n",
        "acc=accuracy_score(pred2,t3)\n",
        "f1=f1_score(pred2,t3)\n",
        "precision=precision_score(pred2,t3)\n",
        "recall=recall_score(pred2,t3)\n",
        "print(\" accuracy={} \\n f1={} \\n precision={} \\n recall={}\".format(acc,f1,precision,recall))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " accuracy=0.9916 \n",
            " f1=0.9936150805716023 \n",
            " precision=0.9927095990279465 \n",
            " recall=0.9945222154595252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nACpRf65tAI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}